在信息论中，熵(entropy)用于表示是接收的每条消息中包含的**信息的平均量**，也可以理解成**随机变量的不确定性**（这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的**信息**）[^1]

## 特性

设X是一个有限状态的离散型随机变量，其概率分布为

$$
P(X=x_i) = p_i,\quad i=1,2,\dots,n
$$
那么随机变量X的熵定义为
$$
H(X) = -\sum_{i=1}^{n}(p_i\log{p_i})
$$
熵越大，那么随机变量的不确定性就越大



## 公式理解

改写一下：将负号移到对数里面
$$
H(X) = \sum_{i=1}^{n}p_i \log\frac{1}{p_i}
$$
现在$\frac{1}{p_i}$表示**不确定性**

取$\log\frac{1}{p_i}$表示这个不确定性需要的比特数字，也就是**复杂程度**。不同的底数表示不同的编码方式。

<img src="https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2024/01/31/image-20240131211815657-f445c5.png" alt="image-20240131211815657" style="zoom:50%;" />

现在乘上$p_i$表示加权，那么熵公式表示的是表示概率的平均复杂程度。



## AI中各种熵的相互关系

![image-20240131210513012](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2024/01/31/image-20240131210513012-4f4598.png)

### 信息熵

$$
H(X)=-\sum_{i}^{n}p(x_{i})\mathrm{log}p(x_{i}) 
$$

### 联合熵


$$
H(X,Y)=-\sum_{x}\sum_{y}p(x,y)\mathrm{log}p(x,y) \\
$$

### 条件熵


$$
\overline{H}(Y|X)=-\sum_{x}\sum_{y}p(x,y)\mathrm{log}p(y|x)  \\
$$

### 互信息（信息增益）

$$
\begin{aligned}I(X,Y)=\sum_{x,y}p(x,y)\text{log}\frac{p(x,y)}
{p(x)p(y)}\end{aligned} \\
$$

### 交叉熵

$$
L(p,q)=-\sum_ip(x_i)\mathrm{log}q(x_i)
$$




### [相对熵](相对熵.md)（KL离散度）

$$
D_{KL}(p\|q)=\sum_{i}p(x_{i})\mathrm{log}\frac{p(x_{i})}{q(x_{i})}
$$



## Reference

[^1]: [熵 (信息论) - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))
[^2]: 【【熵】混沌世界的秩序，不确定性的确定】https://www.bilibili.com/video/BV1jk4y1N7W3?vd_source=62db977db04c368f6f444eed1e6afa65