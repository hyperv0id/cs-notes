## 二元分类

**问题类型**：Logistic Regression用于二元分类（binary classification）。

- 输入：一个特征向量x（如图像）。
- 输出：标签y（1表示正类，如“猫”；0表示负类，如“非猫”）。

示例：**识别猫图片**。

- 输入图像：64x64像素，包含红（R）、绿（G）和蓝（B）三个颜色通道。
- 图像存储：计算机使用3个64x64矩阵表示R、G、B通道的像素强度值。
- 特征向量转换：将像素值“展开”（unroll）成一个一维向量x。
  - 过程：先列出所有R通道像素（e.g., 255, 231, ...），然后G通道，最后B通道。
  - 维度：x的长度为64 * 64 * 3 = 12,288。
  - 符号：nx = 12,288（输入特征向量的维度）；有时简写为n。



![image-20250424093719029](http://assets.hypervoid.top/img/2025/04/24/image-20250424093719029-b79a.png)





## Logistic Regression概述

**定义**：Logistic Regression是一种监督学习算法，用于二元分类问题（输出为0或1）。

输出预测：

- Y hat（Ŷ）表示算法的预测输出，是Y等于1的概率。
- 形式化：Ŷ = P(Y = 1 | X)，即给定输入X，Y为1的概率。
- 示例：如果X是图片，Ŷ表示“这是猫图片的概率”。

输入和参数

- X：nx维特征向量（如之前视频中提到的图像像素向量）。
- 参数：W（nx维向量）和b（一个实数）。

### 2. 为什么不使用线性函数？

- 问题

  ：简单地将Ŷ = W^T X + b（如线性回归）会产生问题，因为：

  - 输出值可能大于1或小于0，不符合概率的定义（概率应在[0, 1]之间）。
  - 示例：W^T X + b 可能为负数或很大，难以解释为概率。

- **原因**：在二元分类中，我们需要确保输出代表**可能性**，而线性函数无法强制输出在[0, 1]范围内。



## 损失函数

### 损失函数的定义

在Logistic Regression中，损失函数用于衡量单个训练样本的预测输出y hat与真实标签y之间的差异。公式为：L(y hat, y) = - [y log(y hat) + (1 - y) log(1 - y hat)]。这个函数不是平方误差，因为平方误差会导致优化问题非凸，从而影响梯度下降的性能。相反，这个损失函数使优化问题变为凸的，便于找到全局最优解。如果y等于1，损失函数会推动y hat接近1；如果y等于0，会推动y hat接近0。这确保了模型在训练样本上表现良好。

### 成本函数的计算

成本函数J(W, b)是损失函数在整个训练集上的平均值。公式为：J(W, b) = (1/m) Σ [ - y(i) log(y hat(i)) + (1 - y(i)) log(1 - y hat(i)) ]，其中m是训练样本数，y hat(i) = σ(W^T X(i) + b)，σ是Sigmoid函数。通过最小化J(W, b)，我们优化参数W和b，使模型在所有训练样本上整体表现最佳。

