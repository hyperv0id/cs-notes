[梯度下降](梯度下降.md)的改进，增加了一个随机值，此外还有小批量随机梯度下降，它可以解决几乎所有的深度学习问题。

**每次迭代时**使用**一个样本**来对参数进行更新。
## 简介

**随机梯度下降**（Stochastic Gradient Descent，**SGD**）在每一次更新参数时都使用一个样本来进行更新，m等于1，更新很多次，有时候也被称为在线梯度下降。当样本数据很大时，可能到迭代完成，也只不过遍历了样本中的一小部分。因此其速度较快，但最终的结果是在全局最优解的附近。对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次，这种跟新方式计算复杂度太高。

​    SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。即使SGD在技术上是指每次使用1个数据来计算梯度，还是会有人使用SGD来指代小批量数据梯度下降。

​    优点：

- 训练速度快
- 适合大样本；
- 提高了泛化误差

​    缺点：

- **准确度**下降，并不是全局最优。
- **噪声**更多。
- 迭代**次数**多，不易于并行实现。

​    其代码段简单在训练样本下的增加了一个循环，计算出对于每个样本下的梯度，每次epoch都shuffle了训练集。

```python
# 随机梯度下降
for i in range(nb_epochs):
 np.random.shuffle(data) # 打乱数据
 for example in data:
        params_grad = evaluate_gradient(loss_function, example,params)
        params = params - learning_rate * params_grad
```
