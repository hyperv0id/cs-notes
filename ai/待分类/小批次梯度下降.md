---
aliases:
  - MBGD
  - Mini-Batch Gradient Descent
  - 小批量梯度下降
tags:
  - ai
  - math/最优化
---
## 简介
兼顾了[批量梯度下降](批量梯度下降.md)的计算效率和[随机梯度下降](随机梯度下降.md)的速度。是机器学习的不二之选，在深度学习中，小批次梯度下降就是随机梯度下降的代名词。

- 小批量：每一步选少量样本
- 学习率：此模型的学习率会随时间不断衰减

## 细节

### 批次多少合适

1. 更大的批量会计算更精确的梯度，但是回报却是小于线性的。
2. 极小的批量通常难以充分利用**多核**结构。当批量低于某个数值时，计算时间不会减少。
3. 批量处理中的所有样本可以并行处理，但是**内存消耗**和批量大小会成正比。
4. 在使用**GPU**时，通常使用**2的幂数**作为批量大小可以获得更少的运行时间。一般，2的幂数取值范围是**32~256**。16有时在尝试大模型时使用。

### 为什么要降低学习率

1. 在梯度下降**初期**，能接受较大的步长（学习率），以较快的速度进行梯度下降。
2. 当**收敛**时，我们希望步长小一点，并且在最小值附近小幅摆动。


常见学习率衰减方法：
> $\beta$为衰减率，$\lambda$ 为epoch数量，$k$ 为常数，$\alpha_0$ 为初始学习率。
$$
\alpha = \frac{\alpha_0}{1+\beta^* \lambda}
$$
$$
\alpha=0.95^{\lambda*}\alpha_{0}
$$
$$
\alpha=\frac{k}{\sqrt{\lambda}}\alpha_{0}
$$
$$
\alpha=\frac k{\sqrt{\text{mini-batch size}}}\alpha_0
$$
