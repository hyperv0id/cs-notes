---
aliases:
  - Fisher判别分析
  - Linear discriminant analysis
  - LDA
tags:
  - ai
---

## 线性判别分析
也叫Fisher判别分析。[罗纳德·费希尔\_百度百科](https://baike.baidu.com/item/%E7%BD%97%E7%BA%B3%E5%BE%B7%C2%B7%E8%B4%B9%E5%B8%8C%E5%B0%94/58187642)

思想是：**类内小，类间大**。



从**降维**的角度出发，把数据全部**投影**到一维的坐标轴上，之后选定一个**阈值**来分类。

![img](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2024/01/11/v2-3b91b15664720a0cb9198bca2c84bb3b_r-68c753.jpeg)

现在需要找一个最合适的**投影方向**，比如说上图中右边的更好。

这里指的就是**最大化类间间隔，最小化类内方差**。用计算机行话就是**“高内聚低耦合”**。

### 模型定义

首先是投影，我们假定原来的数据是向量 $x$，那么顺着 $ w$ 的**方向投影**就是标量$z$：
$$
z=w^T\cdot x(=|w|\cdot|x|\cos\theta)
$$
投影的**均值**$\bar{z}$：
$$
\bar{z}= \frac{1}{N}w^T\cdot x
$$


投影的**方差**$S_z$：
$$
\begin{align}
S_z &=\frac{1}{N}\sum\limits_{i=1}^{N_2}(z_i-\bar{z})(z_i-\bar{z})^T\nonumber\\
&=\frac{1}{N}\sum\limits_{i=1}^{N_2}(w^Tx_i-\bar{z})(w^Tx_i-\bar{z})^T\nonumber
\end{align}
$$
假设属于两类的试验样本数量分别是 $N_1$和 $N_2$，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了协方差的定义，用 $S$ 表示原数据的协方差：
$$
\begin{align}
C_1:Var_z[C_1]&=\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(z_i-\overline{z_{c1}})(z_i-\overline{z_{c1}})^T\nonumber\\
&=\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(w^Tx_i-\frac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\frac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)^T\nonumber\\
&=w^T\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(x_i-\overline{x_{c1}})(x_i-\overline{x_{c1}})^Tw\nonumber\\
&=w^TS_1w
\end{align}
$$

$$
\begin{align}
C_2:Var_z[C_2]&=\frac{1}{N_2}\sum\limits_{i=1}^{N_2}(z_i-\overline{z_{c2}})(z_i-\overline{z_{c2}})^T\nonumber\\
&=w^TS_2w
\end{align}
$$



所以**类内**距离可以记为**方差的和**：
$$
\begin{align}
Var_z[C_1]+Var_z[C_2]=w^T(S_1+S_2)w
\end{align}
$$
对于**类间**距离，我们可以用两类的**均值**表示这个距离：
$$
\begin{align}
(\overline{z_{c1}}-\overline{z_{c2}})^2&=(\frac{1}{N_1}\sum\limits_{i=1}^{N_1}w^Tx_i-\frac{1}{N_2}\sum\limits_{i=1}^{N_2}w^Tx_i)^2\nonumber\\
&=(w^T(\overline{x_{c1}}-\overline{x_{c2}}))^2\nonumber\\
&=w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw
\end{align}
$$
综合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值：
$$
\begin{align}
\hat{w}=\mathop{argmax}\limits_wJ(w)&=\mathop{argmax}\limits_w\frac{(\overline{z_{c1}}-\overline{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\nonumber\\
&=\mathop{argmax}\limits_w\frac{w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw}{w^T(S_1+S_2)w}\nonumber\\
&=\mathop{argmax}\limits_w\frac{w^TS_bw}{w^TS_ww}
\end{align}
$$


> 这里
>
> - $S_b$(Between Class)表示类间方差
>
> - $S_w$(Within Class)表示类内方差

### 模型求解

这样，我们就把损失函数和原数据集以及参数结合起来了。



下面对这个损失函数求偏导，注意我们其实对 $w$ 的绝对值没有任何要求，只对**方向**有要求，因此只要一个方程就可以求解了：
$$
\begin{align}
&\frac{\partial}{\partial w}J(w)=2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww=0\nonumber\\
&\Longrightarrow S_bw(w^TS_ww)=(w^TS_bw)S_ww\nonumber\\
&\Longrightarrow w\propto S_w^{-1}S_bw=S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw\propto S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})
\end{align}
$$
于是 $ S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})$ 就是我们需要寻找的方向。最后可以归一化求得单位的 $w$ 值。