---
aliases:
  - Least squares
tags:
  - ai
  - math
---

# 最小二乘法

假设采用二范数定义的平方误差来定义损失函数：
$$
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2
$$
展开得到：
$$
\begin{align}
L(w)&=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)\cdot (w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\nonumber\\
&=(w^TX^T-Y^T)\cdot (Xw-Y)=w^TX^TXw-Y^TXw-w^TX^TY+Y^TY\nonumber\\
&=w^TX^TXw-2w^TX^TY+Y^TY
\end{align}
$$
现在要**最小化**这个值的 $ \hat{w}$ ：
$$
\begin{align}
\hat{w}=\mathop{argmin}\limits_wL(w)&\longrightarrow\frac{\partial}{\partial w}L(w)=0\nonumber\\
&\longrightarrow2X^TX\hat{w}-2X^TY=0\nonumber\\
&\longrightarrow \hat{w}=(X^TX)^{-1}X^TY=X^+Y
\end{align}
$$
这个式子中 $(X^TX)^{-1}X^T$ 又被称为**伪逆**。对于行满秩或者列满秩的 $X$，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法，对 $X$ 求奇异值分解，得到
$$
X=U\Sigma V^T
$$
于是：
$$
X^+=V\Sigma^{-1}U^T
$$
在几何上，最小二乘法相当于模型（这里就是直线）和试验值的距离的平方求和，假设我们的试验样本张成一个 $p$ 维空间（满秩的情况）：$X=Span(x_1,\cdots,x_N)$，而模型可以写成 $f(w)=X\beta$，也就是 $x_1,\cdots,x_N$ 的某种组合，而最小二乘法就是说希望 $Y$ 和这个模型距离越小越好，于是它们的差应该与这个张成的空间垂直：
$$
X^T\cdot(Y-X\beta)=0\longrightarrow\beta=(X^TX)^{-1}X^TY
$$



## 向量空间视角

总误差分散在N个样本点上，最小二乘法使得我们拟合出的曲线总误差最小。



第二种解释：将原来的函数改写成：
$$
f(w) = w^Tx = x^T\beta
$$
这就是一个$p$维空间，使得每个样本点尽可能地在这个空间中

![image-20240110220029348](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2024/01/10/image-20240110220029348-eb77bf.png)

每个样本点拟合的值不一定在$p$维空间中，这时候做一个法向量$Y-x\beta$，所有的法向量都和$X$矩阵垂直。

那么现在得出结论：
$$
x^T(Y-x\beta) = 0
$$
即：
$$
\begin{align}
X^T(Y-x\beta) &= 0 \\
X^TY &= X^Tx\beta \\
\beta &= (X^Tx)^{-1}X^TY
\end{align}
$$
![image-20240110220302262](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2024/01/10/image-20240110220302262-3118a7.png)

## 概率视角

### 噪声成高斯分布（MLE）

最幸运的情况：所有数据都在一个直线上。

但是现实生活中的数据有**噪声**，这里假设噪声的概率服从一个高斯分布。

对于一维的情况，记 $y=w^Tx+\epsilon,\epsilon\sim\mathcal{N}(0,\sigma^2)$，那么 $y\sim\mathcal{N}(w^Tx,\sigma^2)$。代入极大似然估计(MLE)中：
$$
\begin{align}
L(w)=\log p(Y|X,w)&=\log\prod\limits_{i=1}^Np(y_i|x_i,w)\nonumber\\
&=\sum\limits_{i=1}^N\log(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\\
\mathop{argmax}\limits_wL(w)&=\mathop{argmin}\limits_w\sum\limits_{i=1^N}(y_i-w^Tx_i)^2
\end{align}
$$
这个表达式和最小二乘估计得到的结果一样。

### 权重先验也为高斯分布（MAP）

取先验分布 $w\sim\mathcal{N}(0,\sigma_0^2)$。于是： 
$$
\begin{align}
\hat{w}=\mathop{argmax}\limits_wp(w|Y)&=\mathop{argmax}\limits_wp(Y|w)p(w)\nonumber\\
&=\mathop{argmax}\limits_w\log p(Y|w)p(w)\nonumber\\
&=\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\nonumber\\
&=\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw]
\end{align}
$$
这里省略了 $X$，$p(Y)$和 $w$ 没有关系，同时也利用了上面高斯分布的 MLE的结果。

我们将会看到，超参数 $\sigma_0$的存在和下面会介绍的 Ridge 正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 L1 正则类似的结果。
