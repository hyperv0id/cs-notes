---
tags:
  - ai/激活函数
---
在神经网络中，激活函数（Activation Function）是一种用于**引入非线性**特性的数学操作。在神经网络的每个神经元上，激活函数将输入信号进行非线性转换，以产生输出。这种非线性变换对于神经网络的学习和表示能力非常重要，因为它允许网络模型捕捉和学习复杂的模式和关系。

激活函数的作用是给神经网络引入非线性因素，使得神经网络能够学习和表示更加复杂的函数。如果没有激活函数，多个线性层的组合仍然会是线性的，无法表达更复杂的关系。激活函数的引入使神经网络能够逼近任意复杂的函数，这是神经网络强大表示能力的基础之一。

常见的激活函数包括：

1. **[Sigmoid函数](待分类/Sigmoid函数.md)（[Logistic函数](待分类/Sigmoid函数.md)）：** 将输入映射到一个范围在0和1之间的连续输出。公式为：\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]

2. **双曲正切函数（Tanh函数）：** 类似于[Sigmoid函数](待分类/Sigmoid函数.md)，但将输出范围映射到-1和1之间。公式为：\[ \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \]

3. **ReLU函数（Rectified Linear Unit）：** 将负数映射到零，正数保持不变。公式为：\[ \text{ReLU}(x) = \max(0, x) \]

4. **Leaky ReLU函数：** 类似于ReLU，但允许小于零的输入有一个小的斜率。这有助于解决ReLU在负数部分可能出现的问题。公式为：\[ \text{Leaky ReLU}(x) = \max(\alpha x, x) \] 其中，\(\alpha\) 是一个小的正数。

5. **softmax函数：** 用于多类别分类问题，将输入转换为表示概率分布的输出。公式为：\[ \text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{N}e^{x_j}} \] 其中，\(N\) 是输出的维度。

选择激活函数通常取决于具体的问题和网络结构，不同的激活函数适用于不同的场景。