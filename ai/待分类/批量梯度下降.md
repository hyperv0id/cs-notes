---
aliases:
  - Batch Gradient Descent
tags:
  - ai
  - math/最优化
---

梯度下降的变体，可以提高速度
### 批量梯度下降

在**每一次迭代时**使用**所有样本**来进行梯度的更新。这个过程称为一个训练周期 (training epoc)。

优点：

1. 在训练过程中，使用**固定的学习率**，不必担心学习率衰退现象的出现。

2. 由全数据集确定的方向能够更好地代表样本总体，从而更**准确**地朝向极值所在的方向。当目标函数为凸函数时，一定能收敛到全局最小值，如果目标函数非凸则收敛到局部最小值。

3. 它对梯度的估计是**无偏**的。样例越多，标准差越低。

4. 一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了**并行**。

缺点：

1. 全量遍历：尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了。

2. 多余参数：每次的更新都是在遍历全部样例之后发生的，这时才会发现一些例子可能是多余的且对参数更新没有太大的作用。