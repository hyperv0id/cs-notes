---
tags:
---

---
<iframe src="https://onedrive.live.com/embed?resid=C5FEA982BBD2F6E%21388850&authkey=!ANsiUq1ZeIquCq0&em=2" width="714" height="432" frameborder="0" scrolling="no"></iframe>


---
## Lec 01

图灵测试：
1. **有限的交互范围**：图灵测试只考察了AI在语言交互上的能力，而忽略了其他许多重要的能力，如视觉识别、运动控制等。
    
2. **欺骗性**：图灵测试的目标是让AI尽可能地模仿人类，这可能导致AI系统采取一些欺骗性的策略，而不是真正地理解和思考问题。
    
3. **缺乏透明度**：图灵测试只关注结果，而不关注过程。一个AI系统可能通过了图灵测试，但我们无法知道它是如何做到的，这使得我们很难评估它的可靠性和健壮性。
## Lec 02

在机器学习中，我们通常会遇到以下几种学习方式：监督学习、无监督学习、强化学习和半监督学习。下面我将详细解释这些学习方式的区别和联系：

1. **监督学习**：在监督学习中，我们有一个标记的数据集，也就是说，我们知道输入数据和对应的输出结果。我们的目标是训练一个模型，使其能够对新的、未标记的数据进行预测。常见的监督学习任务包括分类（如判断一封邮件是否为垃圾邮件）和回归（如预测房价）。

2. **无监督学习**：与监督学习不同，无监督学习的数据集没有标签，我们的目标是找出数据中的某种结构或模式。常见的无监督学习任务包括聚类（如将客户分为不同的群体）和降维（如通过PCA降低数据的维度）。

3. **强化学习**：在强化学习中，我们有一个智能体（agent）在环境中进行操作，通过与环境的交互获得奖励或惩罚。智能体的目标是学习一种策略，使其能够在一系列决策中获得最大的总奖励。强化学习常用于游戏、机器人控制等领域。

4. **半监督学习**：半监督学习介于监督学习和无监督学习之间。在半监督学习中，我们有一部分数据是标记的，一部分数据是未标记的。我们的目标是利用这两部分数据，训练出一个更好的模型。半监督学习常用于当我们有大量的未标记数据，但只有少量的标记数据时。

这四种学习方式各有优势，适用于不同的场景。在实际应用中，我们可能会根据具体的问题和数据情况，选择最适合的学习方式。

## Lec 03

看视频：[CSCI 6350 Artificial Intelligence: Minimax and Alpha-Beta Pruning Algorithms and Psuedocodes - YouTube](https://youtu.be/J1GoI5WHBto?t=1818)

## Lec 04
### 适应度函数

在遗传算法中，适应度函数（fitness function）是一个非常关键的组成部分，它用于评估每个个体（解决方案）的质量。适应度函数的定义会直接影响遗传算法的搜索过程和最终结果。

#### 定义适应度函数

适应度函数的定义应该与你要解决的问题紧密相关。通常，适应度函数会根据问题的特性和目标进行设计。例如，如果你的目标是最小化某个函数，那么适应度函数可以定义为该函数的倒数；如果目标是最大化某个函数，那么适应度函数可以直接使用该函数。

适应度函数的设计需要满足以下几个原则：

1. **反映目标**：适应度函数应该能够准确地反映出你的优化目标。也就是说，一个个体的适应度值应该与其质量成正比。

2. **区分度**：适应度函数应该能够区分不同的个体。如果所有个体的适应度值都相同，那么遗传算法就无法进行有效的搜索。

3. **连续性**：理想情况下，适应度函数应该是连续的，也就是说，相似的个体应该有相似的适应度值。这样可以保证遗传算法的搜索过程是平滑的，而不是跳跃的。

#### 适应度函数对结果的影响

适应度函数的定义会直接影响遗传算法的搜索过程和最终结果。一个好的适应度函数可以引导遗传算法快速地找到优秀的解决方案，而一个不好的适应度函数可能会使遗传算法陷入局部最优，或者无法收敛。

### 种群大小对结果的影响

种群大小（population size）也是遗传算法中的一个重要参数，它决定了每一代有多少个体。种群大小的选择会影响遗传算法的搜索能力和计算复杂性。

1. **搜索能力**：种群大小越大，遗传算法的搜索能力越强。因为有更多的个体，遗传算法可以探索更多的解决方案空间，从而有更大的可能性找到优秀的解决方案。

2. **计算复杂性**：然而，种群大小越大，遗传算法的计算复杂性也越高。因为每一代都需要计算所有个体的适应度值，所以如果种群大小过大，可能会导致计算成本过高。

因此，选择合适的种群大小是一个权衡搜索能力和计算复杂性的过程。在实际应用中，我们通常会通过实验来确定最佳的种群大小。



## Lec 05

就像 算法——代码——内存之间的关系

![image.png](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2024/05/03/20240503230008-ee48ed.png)

## Lec 06

会员函数（Membership Function）是模糊集合理论中的一个重要概念，它用于描述一个元素属于某个模糊集合的程度。

在模糊逻辑中，一个元素不是简单地属于或不属于一个集合，而是有一定的属于程度，这个属于程度就是由会员函数来描述的。会员函数的值在0和1之间，0表示元素完全不属于该集合，1表示元素完全属于该集合，而在0和1之间的值则表示元素在一定程度上属于该集合。

例如，我们可以定义一个“高个子”模糊集合，对于一个人的身高，我们可以定义一个会员函数，使得身高较低的人属于“高个子”集合的程度较低，而身高较高的人属于“高个子”集合的程度较高。

会员函数在模糊逻辑和模糊推理中起着关键作用，它使得我们可以在处理不确定性和非精确性的问题时，有更多的灵活性和表达能力。



这些都是模糊集合理论中的一些基本概念，用于描述模糊集合的特性：

1. **支持（Support）**：支持是指所有隶属度非零的元素的集合。在模糊集合中，支持包含了所有在某种程度上属于该集合的元素。

2. **核（Core）**：核是指所有隶属度等于1的元素的集合。在模糊集合中，核包含了所有完全属于该集合的元素。

3. **α-Cut**：α-Cut是指所有隶属度大于或等于α的元素的集合。通过α-Cut，我们可以得到模糊集合在不同隶属度水平下的元素集合。

4. **高度（Height）**：高度是指模糊集合中的最大隶属度。在模糊集合中，高度可以用来衡量集合的模糊程度，高度越高，集合的模糊程度越低。

这些概念在模糊逻辑和模糊推理中都有重要的应用，它们帮助我们更好地理解和描述模糊集合的特性。


会员函数（Membership Function，MF）和概率密度函数（Probability Density Function，PDF）都是用于描述数据的分布和特性的数学工具，但它们的目标和应用场景有所不同。

**会员函数**是模糊集合理论中的一个概念，用于描述一个元素属于某个模糊集合的程度。会员函数的值在0和1之间，0表示元素完全不属于该集合，1表示元素完全属于该集合，而在0和1之间的值则表示元素在一定程度上属于该集合。会员函数在模糊逻辑和模糊推理中起着关键作用，它使得我们可以在处理不确定性和非精确性的问题时，有更多的灵活性和表达能力。

**概率密度函数**则是概率论和统计学中的一个概念，用于描述一个连续随机变量的概率分布。概率密度函数的值在任何一点上都是非负的，而且其在整个定义域上的积分等于1。概率密度函数可以用来计算随机变量落在某个区间内的概率，从而帮助我们理解和预测随机现象。

总的来说，会员函数和概率密度函数都是描述数据特性的工具，但会员函数更注重描述元素的隶属度，适用于模糊逻辑和模糊推理，而概率密度函数更注重描述随机变量的概率分布，适用于概率论和统计学。


## Lec 07

离线算法、K需要预先给定

K-means是一种非常流行的聚类算法，它的目标是将数据点分配到K个聚类中，使得每个数据点都属于离它最近的聚类中心（也称为质心）的聚类。以下是K-means算法的基本步骤：

1. **初始化**：选择K个数据点作为初始的聚类中心。

2. **分配数据点**：对于每个数据点，计算它到所有K个聚类中心的距离，然后将它分配到距离最近的聚类中。

3. **更新聚类中心**：对于每个聚类，计算聚类中所有数据点的平均值，然后将这个平均值作为新的聚类中心。

4. **重复步骤2和3**：重复执行步骤2和3，直到聚类中心不再发生变化，或者达到预设的最大迭代次数。

K-means算法简单易用，但也有一些局限性。例如，它假设所有的聚类都是**凸形**的和**对称**的，这在某些应用中可能不成立。此外，K-means算法的结果可能会**受到初始聚类中心**选择的影响，因此通常需要多次运行算法，或者使用一些技巧来选择好的初始聚类中心。


Mini-Batch K-means算法：kmeans的在线改进版本


## Lec 08

分类和回归的区别

分类：目标是一个离散值（类别
回归：目标是一个量（房价

过拟合：
- 原因
	1. 模型复杂度过高
	2. 数据不足
	3. 特征选择
	4. 噪声
- 三个解决方式：
	1. 加数据
	2. 特征选择（降低特征维度）如 PCA 算法。
	3. [正则化](正则化.md)
欠拟合：算法太垃圾、模型太简单（用线性拟合非线性）、特征选择不当、训练数据不足


**k-近邻算法**（k-NN）是一种基于实例的学习，或者说是懒惰学习的算法，因为它实际上并不从训练集中学习一个判别函数，而是把训练集的所有数据都存储起来，预测时再进行处理。因此，k-NN可以被看作是一种离线算法，因为它需要在预测时访问全部（或者说大部分）训练数据。

**k值的选择**对k-NN的效果有很大影响。如果k值选择得太小，那么模型可能会过于复杂，容易受到噪声的影响；如果k值选择得太大，那么模型可能会过于简单，不能很好地捕捉到数据的局部结构。在实际应用中，k值通常通过交叉验证等方法来选择。

**k-NN和k-means的主要区别**在于它们的目标：k-NN是一种监督学习算法，用于分类或回归；而k-means是一种无监督学习算法，用于聚类。在k-NN中，k是指在预测新样本的类别或值时，考虑的最近邻样本的数量；而在k-means中，k是指要将数据划分成的聚类的数量。

## Lec 09
不考
## Lec 10

贝叶斯网络
![image.png](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2024/05/05/20240505163649-f391eb.png)

案例：[(ML 13.12) How to use D-separation - illustrative examples (part 1) - YouTube](https://www.youtube.com/watch?v=aA-gTNxy1rw)

计算概率

## Lec 11

MDP：定义（SAPRY是什么）、计算（贝尔曼方程表示平均收益）

![image.png](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2024/05/05/20240505164925-7f274f.png)


## Lec 13

- [CNN](CNN.md)
- [RNN](RNN.md)

## Lec 14

on-device learning好处
模型轻量化、怎么知识蒸馏

