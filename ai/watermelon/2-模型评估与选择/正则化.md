在实际应用时，如果样本容量不远远大于样本的特征维度，很可能造成[过拟合](过拟合.md)，对这种情况，我们有下面三个解决方式：

1. 加数据
    
2. 特征选择（降低特征维度）如 PCA 算法。
    
3. 正则化
    

正则化一般是在损失函数（如上面介绍的最小二乘损失）上加入**正则化项**（表示模型的复杂度对模型的惩罚），下面我们介绍一般情况下的两种正则化。

$$  
\begin{align} L1&:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\\ L2&:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0 \end{align}  
$$

## 简介


目的：防止过拟合，增大模型的稳定性。

$$
L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\
$$



- L：正则化强度
- 数据损失：拟合我们的数据的误差
- 正则化损失：避免在数据集上做的太好
- 正则化
  - 目的：提高模型的泛化能力
  - L1：$R(W) = \sum_k\sum_lW^2_{k,l}$
  - L2：$R(W) = \sum_k\sum_l{|W_{k, l}|}$
  - 弹性网络：综合考虑L1L2，$R(W) = \sum_k\sum_l\{{\beta W^2_{k,l}+|W_{k, l}|\}}$
  - 比正则化更复杂的泛化技巧：dropout、batch normalization、stochastic depth、fractional pooling


## 正则化例子
### L1正则化

也叫做Lasso。

L1正则化可以引起稀疏解。

从最小化损失的角度看，由于 L1 项求导在0附近的左右导数都不是0，因此更容易取到0解。

从另一个方面看，L1 正则化相当于：

$$  
\mathop{argmin}\limits_wL(w)\\ s.t. ||w||_1\lt C  
$$

  

我们已经看到平方误差损失函数在 w 空间是一个椭球，因此上式求解就是椭球和 ||w||_1=C的切点，因此更容易相切在坐标轴上。

### L2 Ridge

也叫做岭回归。

$$  
\begin{align} \hat{w}=\mathop{argmin}\limits_wL(w)+\lambda w^Tw&\longrightarrow\frac{\partial}{\partial w}L(w)+2\lambda w=0\nonumber\\ &\longrightarrow2X^TX\hat{w}-2X^TY+2\lambda \hat w=0\nonumber\\ &\longrightarrow \hat{w}=(X^TX+\lambda \mathbb{I})^{-1}X^TY \end{align}  
$$

  

可以看到，这个正则化参数和前面的 MAP 结果不谋而合。利用2范数进行正则化不仅可以是模型选择 w 较小的参数，同时也避免  X^TX不可逆的问题。