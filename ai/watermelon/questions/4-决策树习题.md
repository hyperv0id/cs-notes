## 4.1

> 试证明对于不含冲突数据(即特征向量完全相同但标记不同)的训练集，必存在与训练集一致(即训练误差为 0) 的决策树.

如果不存在冲突数据，那么就可以构造一个和训练集一样的假设空间，根本就不需要进行划分，直接组合就行



## 4.2

> 试析使用"最小训练误差"作为决策树划分选择准则的缺陷

[过拟合](../2-模型评估与选择/过拟合.md)



## 4.3

> 试编程实现基于信息;嘀进行划分选择的决策树算法?并为表 4.3 中数据生成一棵决策树.



## 4.4

> 试编程实现基于基尼指数进行划分选择的决策树算法，为表 4.2 中数据生成预剪枝、后剪枝决策树?并与未剪枝决策树进行比较.



## 4.5

> 试编程实现基于对率回归进行划分选择的决策树算法?并为表 4.3 中数据生成一棵决策树.

## 4.6

> 试选择 4 个 UCI 数据集，对上述 3 种算法所产生的未剪枝、预剪枝、后剪枝决策树进行实验比较，并进行适当的统计显著性检验.



## 4.7

> 图 4.2 是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致"栈"溢出试使用"队列"数据结构，以参数MaxDepth 控制树的最大深度，写出与图 4.2 等价、但不使用递归的决策树生成算法.



## 4.8*

> 试将决策树生成的深度优先搜索过程修改为广度优先搜索，以参数MαxNode 控制树的最大结点数，将题 4.7 中基于队列的决策树算法进行改写.对比题 4.7 中的算法，试析哪种方式更易于控制决策树所需存储不超出内存.

## 4.9

> 试将 4.4.2 节对缺失值的处理机制推广到基尼指数的计算中去.



## 4.10

> 从网上下载或自己编程实现任意一种多变量决策树算法，并观察其在西瓜数据集 3.0 上产生的结果.