## 3.1

> 什么情况下式(3.2)中不需要考虑偏置项

已经知道 $y$ 是 $x$ 的正比例函数

## 3.2

> 证明参数$w$，对率回归的目标函数非凸，但对数似然函数是凸的

对率回归：
$$
y = \frac{1}{1+e^{-(w^Tx+b)}} \tag{3.18}
$$

对数似然函数
$$
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(-y_{i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}+\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}}\right)\right) \tag{3.27}
$$


> 对实数集上的函数，可通过求二阶导数来在判别，若二阶导数在区间中非负，则称为凸函数，若二阶导数在区间上恒大于 0，则称为严格凸函数.(p54)

现在对对率回归的 $w$ 求二阶偏导：
$$
\frac{\partial}{\partial w}
\left(\frac{1}{1+e^{-w^T x+b}}\right)
=
\frac{e^{-w^Tx+b_{x}}}{\left(1+e^{-w^T x +b}\right)^{2}}
$$

$$
\frac{\partial}{\partial w^2} = \frac{-x^2 e^{-w^T x+b}\times [1+e^{-(w^T+b)}]^2}{[1-e^{-(w^T+b)}]^4} < 0
$$

对对数似然函数的 $w$ 求二阶偏导

书上(3.30)和(3.31写了)

## 3.3

> 编程实现对率回归，并给出西瓜数据集 $3.0 \alpha$ 上的结果.



## 3.4

> 选择两个 UCI 数据集，比较 10 折交叉验证法和留法所估计出的对率回归的错误率.

### Lris数据集
数据集地址：[UCI Machine Learning Repository: Iris Data Set](http://archive.ics.uci.edu/ml/datasets/Iris)


## 3.5

> 编辑实现线性判别分析，并给出西瓜数据集 $3.0α$ 上的结果.



## 3.6

> 线性判别分析仅在线性可分数据上能获得理想结果?试设计一个改进方法，使其能较好地周于非线性可分数据



## 3.7

> 令码长为 9，类别数为 4，试给出海明距离意义下理论最优的 ECOC二元码井证明之.



## 3.8

> ECOC 编码能起到理想纠错作用的重要条件是:在每一位编码上出错的概率相当且独立.试析多分类任务经 ECOC 编码后产生的二类分类器满足该条件的可能性及由此产生的影响.



## 3.9

> 使用 OvR 和 MvM 将多分类任务分解为二分类任务求解时，试述为何无需专门针对类别不平衡性进行处理.



## 3.10

> 试推导出多分类代价敏感学习(仅考虑基于类别的误分类代价)使用"再缩放"能获得理论最优解的条件.