# 网络设计的技巧

## 局部最值（Local Minimum）与鞍点（Saddle Point）

在训练过程中，不可避免的会有局部最小值和鞍点的问题。它们的梯度都为0，我们把这两种点统称为临界点（Critical Point）

![img](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2022/11/24/694210e6c04f8e0513e4a13255456fc8a906f7b3_2_661x500-a78e94.png)



那么如何分辨临界点呢

我们可以在$\theta$附近找一点$\theta^\prime$，通过泰勒展开，在$\theta ^ \prime$点逼近$L(\theta)$

L(θ)≈L(θ′)+(θ−θ′)Tg+0.5(θ−θ′)TH(θ−θ′)

梯度$g$为0，那么

L(θ)≈L(θ′)+0.5(θ−θ′)TH(θ−θ′)

当$H$为正定矩阵（所有特征值为正）时，$\theta ^ \prime$为局部最小值

如果有正有负那么为鞍点。

### 处理鞍点

我们需要梯度朝向损失函数小的地方，所以使用负特征值对应特征向量$u$作为梯度，那么$\theta=\theta ^ \prime+u$

![img](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2022/11/24/8da92438659f093131c7805b86508b64dd13a421-2af66f.png)

### 处理局部最小值

1. 局部最小值极少出现
2. 可以通过升维转化为鞍点

## 批次（batch）和动量（Momentum）

**批次**：将数据分为多段，分批训练网络

**批次优势**：即使有一个段是局部最小值，其他部分不是，那么就不是局部最小值

**批次大小**：

1. 在并行计算情况下，大批次更快
2. 小批次更加稳定，泛化率高

**Momentum方法**：$m_1 = \lambda m_0 - ηg_0$，$\theta = \theta^\prime + m_1$

## 自动调整学习速率

Loss一直在高位的原因可能是步子迈的太大在山谷间来回震荡。但步子迈得太小又会降低效率，于是我们需要自适应梯度。

如下图，下一步有梯度和学习率共同决定，随迭代次数逐渐增加，梯度会越来越小。（类似模拟退火）

当然为了防止陷入局部最小值，我们可以在学习率十分低时突然清零，跳出局部，看看最后收敛是否还在原来位置。

我们还增加了权值$\alpha$，使梯度由当前梯度和历史梯度共同决定

![img](https://pic-1257412153.cos.ap-nanjing.myqcloud.com/images/2022/11/24/4813812dbf8c1eeb11d36b601b154118dc474202-e2adb3.png)

## Batch Normalization

归一化，略