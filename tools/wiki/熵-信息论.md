在信息论中，熵(entropy)用于表示是接收的每条消息中包含的**信息的平均量**，也可以理解成**随机变量的不确定性**（这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的**信息**）[^1]

## 特性

设X是一个有限状态的离散型随机变量，其概率分布为

$$
P(X=x_i) = p_i,\quad i=1,2,\dots,n
$$
那么随机变量X的熵定义为
$$
H(X) = -\sum_{i=1}^{n}(p_i\log{p_i})
$$
熵越大，那么随机变量的不确定性就越大

[^1]: [熵 (信息论) - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))